{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Step1_Colab_TensorRT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tony607/tf_jetson_nano/blob/master/Step1_Colab_TensorRT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "QQK7KlhRBfnf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 1 : Convert Keras model into TensorRT model\n",
        "**For more detail, checkout [How to run Keras model on Jetson Nano](https://www.dlology.com/blog/how-to-run-keras-model-on-jetson-nano/) | DLology Blog**"
      ]
    },
    {
      "metadata": {
        "id": "ITUc4J7etACS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow.contrib.tensorrt as trt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VsZgX4XeuNkt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download an image for prediction.\n",
        "\n",
        "!wget --quiet https://raw.githubusercontent.com/Tony607/tf_jetson_nano/master/data/elephant.jpg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fWygvIyctpeI",
        "colab_type": "code",
        "outputId": "d2dea334-f704-4658-a495-c3c9d559e472",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2 as Net\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Optional image to test model prediction.\n",
        "img_path = './elephant.jpg'\n",
        "model_path = './model'\n",
        "\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "# Path to save the model h5 file.\n",
        "model_fname = os.path.join(model_path, 'model.h5')\n",
        "\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "img_height = 224\n",
        "\n",
        "model = Net(weights='imagenet')\n",
        "\n",
        "\n",
        "# Load the image for prediction.\n",
        "img = image.load_img(img_path, target_size=(img_height, img_height))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "preds = model.predict(x)\n",
        "# decode the results into a list of tuples (class, description, probability)\n",
        "# (one such list for each sample in the batch)\n",
        "print('Predicted:', decode_predictions(preds, top=3)[0])\n",
        "# Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]\n",
        "\n",
        "# Save the h5 file to path specified.\n",
        "model.save(model_fname)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Predicted: [('n02504458', 'African_elephant', 0.705134), ('n01871265', 'tusker', 0.1551433), ('n02504013', 'Indian_elephant', 0.039011613)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OY9k1o9iulZF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Benchmark Keras prediction speed."
      ]
    },
    {
      "metadata": {
        "id": "kGqN3UXquW-m",
        "colab_type": "code",
        "outputId": "450a9e18-310e-4ec7-a6ec-34f508e743c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "times = []\n",
        "for i in range(20):\n",
        "    start_time = time.time()\n",
        "    preds = model.predict(x)\n",
        "    delta = (time.time() - start_time)\n",
        "    times.append(delta)\n",
        "mean_delta = np.array(times).mean()\n",
        "fps = 1/mean_delta\n",
        "print('average(sec):{},fps:{}'.format(mean_delta,fps))\n",
        "\n",
        "# Clear any previous session.\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average(sec):0.016628634929656983,fps:60.137227393002135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FBsv7z2WukCz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Freeze graph, generate `.pb` file.\n",
        "Take a notes of the input and output nodes names printed in the output, we will need them when converting `TensorRT` graph and prediction.\n",
        "\n",
        "For Keras MobileNetV2, they are,\n",
        "```\n",
        "['input_1'] ['Logits/Softmax']\n",
        "```\n"
      ]
    },
    {
      "metadata": {
        "id": "L-Jx1Yq0uejv",
        "colab_type": "code",
        "outputId": "1f71e55c-8c33-4d66-92da-1240036c8bd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "cell_type": "code",
      "source": [
        "# force reset ipython namespaces\n",
        "%reset -f\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import graph_io\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "# Clear any previous session.\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "save_pb_dir = './model'\n",
        "model_fname = './model/model.h5'\n",
        "def freeze_graph(graph, session, output, save_pb_dir='.', save_pb_name='frozen_model.pb', save_pb_as_text=False):\n",
        "    with graph.as_default():\n",
        "        graphdef_inf = tf.graph_util.remove_training_nodes(graph.as_graph_def())\n",
        "        graphdef_frozen = tf.graph_util.convert_variables_to_constants(session, graphdef_inf, output)\n",
        "        graph_io.write_graph(graphdef_frozen, save_pb_dir, save_pb_name, as_text=save_pb_as_text)\n",
        "        return graphdef_frozen\n",
        "\n",
        "# This line must be executed before loading Keras model.\n",
        "tf.keras.backend.set_learning_phase(0) \n",
        "\n",
        "model = load_model(model_fname)\n",
        "\n",
        "session = tf.keras.backend.get_session()\n",
        "\n",
        "input_names = [t.op.name for t in model.inputs]\n",
        "output_names = [t.op.name for t in model.outputs]\n",
        "\n",
        "# Prints input and output nodes names, take notes of them.\n",
        "print(input_names, output_names)\n",
        "\n",
        "frozen_graph = freeze_graph(session.graph, session, [out.op.name for out in model.outputs], save_pb_dir=save_pb_dir)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "['input_1'] ['Logits/Softmax']\n",
            "WARNING:tensorflow:From <ipython-input-5-ff288e1df6a1>:15: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.compat.v1.graph_util.remove_training_nodes\n",
            "WARNING:tensorflow:From <ipython-input-5-ff288e1df6a1>:16: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.compat.v1.graph_util.extract_sub_graph\n",
            "INFO:tensorflow:Froze 262 variables.\n",
            "INFO:tensorflow:Converted 262 variables to const ops.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "38l-D915vahb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Optimize with TensorRT\n",
        "Save the result as `.pb` file.\n",
        "\n",
        "*Notes: optimizing TensorRT graph can also be executed on Jetson Nano, but it is very slow.*"
      ]
    },
    {
      "metadata": {
        "id": "g9qanB3Du2i-",
        "colab_type": "code",
        "outputId": "57f5f966-3b13-468d-de5e-fd812710027a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow.contrib.tensorrt as trt\n",
        "\n",
        "trt_graph = trt.create_inference_graph(\n",
        "    input_graph_def=frozen_graph,\n",
        "    outputs=output_names,\n",
        "    max_batch_size=1,\n",
        "    max_workspace_size_bytes=1 << 25,\n",
        "    precision_mode='FP16',\n",
        "    minimum_segment_size=50\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running against TensorRT version 0.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Azhh5OA2vI72",
        "colab_type": "code",
        "outputId": "0aa4643a-42cc-46de-df0a-97829cd9931a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "graph_io.write_graph(trt_graph, \"./model/\",\n",
        "                     \"trt_graph.pb\", as_text=False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./model/trt_graph.pb'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "NLg8O3dfvOum",
        "colab_type": "code",
        "outputId": "f7f6f202-e10f-4477-b035-a0f3149eda8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "!ls model -alh"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 42M\n",
            "drwxr-xr-x 2 root root 4.0K Apr 13 05:02 .\n",
            "drwxr-xr-x 1 root root 4.0K Apr 13 05:01 ..\n",
            "-rw-r--r-- 1 root root  14M Apr 13 05:01 frozen_model.pb\n",
            "-rw-r--r-- 1 root root  14M Apr 13 05:01 model.h5\n",
            "-rw-r--r-- 1 root root  14M Apr 13 05:02 trt_graph.pb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jPVLp1FKvnSF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download the tensorRT graph `.pb` file from colab to your local machine."
      ]
    },
    {
      "metadata": {
        "id": "ZrHyjN_Cvk4Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('./model/trt_graph.pb')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Hg6qDGIwmQn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Next step**: transfer the `trt_graph.pb` to your Jetson Nano, load it up and make predictions.\n",
        "\n",
        "\n",
        "`Step2_keras-jetson-ImageNet-predict.ipynb`"
      ]
    },
    {
      "metadata": {
        "id": "4b6xtTKp_ve7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}