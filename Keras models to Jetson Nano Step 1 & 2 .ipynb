{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is a break down of how to make it happen.\n",
    "\n",
    "Freeze Keras model to TensorFlow graph then creates inference graph with TensorRT.\n",
    "Loads the TensorRT inference graph on Jetson Nano and make predictions.\n",
    "We will do the first step on a development machine since it is computational and resource intensive way beyond what Jetson Nano can handle.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Jetson Nano\n",
    "Follow the official getting started guide to flash the latest SD card image, setup, and boot.\n",
    "\n",
    "One thing to keep in mind, Jetson Nano doesn't come with WIFI radio as the latest Raspberry Pi does, so it is recommended to have a USB WIFI dongle like this one ready unless you plan to hardwire its ethernet jack instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install TensorFlow on Jetson Nano\n",
    "There is a thread on the Nvidia developer forum about official support of TensorFlow on Jetson Nano, here is a quick run down how you can install it.\n",
    "\n",
    "Start a terminal or SSH to your Jetson Nano, then run those commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo apt update\n",
    "\n",
    "sudo apt install python3-pip libhdf5-serial-dev hdf5-tools\n",
    "\n",
    "pip3 install --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v42 tensorflow-gpu==1.13.1+nv19.3 --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you get into the error below,\n",
    "\n",
    "Cannot compile 'Python.h'. Perhaps you need to install python-dev|python-devel\n",
    "Try run\n",
    "\n",
    "sudo apt install python3.6-dev\n",
    "\n",
    "The Python3 might gets updated to a later version in the future. You can always check your version first with python3 --version, and change the previous command accordingly.\n",
    "\n",
    "It is also helpful to install Jupyter Notebook so you can remotely connect to it from a development machine.\n",
    "\n",
    "pip3 install jupyter\n",
    "\n",
    "Also, notice that Python OpenCV version 3.3.1 is already installed which ease a lot of pain from cross compiling. You can verify this by importing the cv2 library from the Python3 command line interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Freeze Keras model and convert into TensorRT model\n",
    "Run this step on your development machine with Tensorflow nightly builds which include TF-TRT by default or run on this Colab notebook's free GPU.\n",
    "\n",
    "First lets loads a Keras model. For this tutorial, we use pre-trained MobileNetV2 came with Keras, feel free to replace it with your custom model when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2 as Net\n",
    "\n",
    "model = Net(weights='imagenet')\n",
    "\n",
    "os.makedirs('./model', exist_ok=True)\n",
    "\n",
    "# Save the h5 file to path specified.\n",
    "model.save(\"./model/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the Keras model save as a single .h5 file, you can freeze it to a TensorFlow graph for inferencing.\n",
    "\n",
    "Take notes of the input and output nodes names printed in the output. We will need them when converting TensorRT inference graph and prediction.\n",
    "\n",
    "For Keras MobileNetV2 model, they are, ['input_1'] ['Logits/Softmax']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import graph_io\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Clear any previous session.\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "save_pb_dir = './model'\n",
    "model_fname = './model/model.h5'\n",
    "def freeze_graph(graph, session, output, save_pb_dir='.', save_pb_name='frozen_model.pb', save_pb_as_text=False):\n",
    "    with graph.as_default():\n",
    "        graphdef_inf = tf.graph_util.remove_training_nodes(graph.as_graph_def())\n",
    "        graphdef_frozen = tf.graph_util.convert_variables_to_constants(session, graphdef_inf, output)\n",
    "        graph_io.write_graph(graphdef_frozen, save_pb_dir, save_pb_name, as_text=save_pb_as_text)\n",
    "        return graphdef_frozen\n",
    "\n",
    "# This line must be executed before loading Keras model.\n",
    "tf.keras.backend.set_learning_phase(0) \n",
    "\n",
    "model = load_model(model_fname)\n",
    "\n",
    "session = tf.keras.backend.get_session()\n",
    "\n",
    "input_names = [t.op.name for t in model.inputs]\n",
    "output_names = [t.op.name for t in model.outputs]\n",
    "\n",
    "# Prints input and output nodes names, take notes of them.\n",
    "print(input_names, output_names)\n",
    "\n",
    "frozen_graph = freeze_graph(session.graph, session, [out.op.name for out in model.outputs], save_pb_dir=save_pb_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally this frozen graph is what you use for deploying. However, it is not optimized to run on Jetson Nano for both speed and resource efficiency wise. That is what TensorRT comes into play, it quantizes the model from FP32 to FP16, effectively reducing the memory consumption. It also fuses layers and tensor together which further optimizes the use of GPU memory and bandwidth. All this come with little or no noticeable reduced accuracy.\n",
    "\n",
    "And this can be done in a single call,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.tensorrt as trt\n",
    "\n",
    "trt_graph = trt.create_inference_graph(\n",
    "    input_graph_def=frozen_graph,\n",
    "    outputs=output_names,\n",
    "    max_batch_size=1,\n",
    "    max_workspace_size_bytes=1 << 25,\n",
    "    precision_mode='FP16',\n",
    "    minimum_segment_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is also a TensorFlow graph but optimized to run on your Jetson Nano with TensorRT. Let's save it as a single .pb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_io.write_graph(trt_graph, \"./model/\",\n",
    "                     \"trt_graph.pb\", as_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the TensorRT graph .pb file either from colab or your local machine into your Jetson Nano. You can use scp/sftp to remotely copy the file. For Windows, you can use WinSCP, for Linux/Mac you can try scp/sftp from the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Loads TensorRT graph and make predictions\n",
    "On your Jetson Nano, start a Jupyter Notebook with command jupyter notebook --ip=0.0.0.0 where you have saved the downloaded graph file to ./model/trt_graph.pb. The following code will load the TensorRT graph and make it ready for inferencing.\n",
    "\n",
    "The output and the input names might be different for your choice of Keras model other than the MobileNetV2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_names = ['Logits/Softmax']\n",
    "input_names = ['input_1']\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def get_frozen_graph(graph_file):\n",
    "    \"\"\"Read Frozen Graph file from disk.\"\"\"\n",
    "    with tf.gfile.FastGFile(graph_file, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    return graph_def\n",
    "\n",
    "\n",
    "trt_graph = get_frozen_graph('./model/trt_graph.pb')\n",
    "\n",
    "# Create session and load graph\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_sess = tf.Session(config=tf_config)\n",
    "tf.import_graph_def(trt_graph, name='')\n",
    "\n",
    "\n",
    "# Get graph input size\n",
    "for node in trt_graph.node:\n",
    "    if 'input_' in node.name:\n",
    "        size = node.attr['shape'].shape\n",
    "        image_size = [size.dim[i].size for i in range(1, 4)]\n",
    "        break\n",
    "print(\"image_size: {}\".format(image_size))\n",
    "\n",
    "\n",
    "# input and output tensor names.\n",
    "input_tensor_name = input_names[0] + \":0\"\n",
    "output_tensor_name = output_names[0] + \":0\"\n",
    "\n",
    "print(\"input_tensor_name: {}\\noutput_tensor_name: {}\".format(\n",
    "    input_tensor_name, output_tensor_name))\n",
    "\n",
    "output_tensor = tf_sess.graph.get_tensor_by_name(output_tensor_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can make a prediction with an elephant picture and see if the model gets it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions\n",
    "\n",
    "# Optional image to test model prediction.\n",
    "img_path = './data/elephant.jpg'\n",
    "\n",
    "img = image.load_img(img_path, target_size=image_size[:2])\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "feed_dict = {\n",
    "    input_tensor_name: x\n",
    "}\n",
    "preds = tf_sess.run(output_tensor, feed_dict)\n",
    "\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark results\n",
    "Let's run the inferencing several times and see how fast it can go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "times = []\n",
    "for i in range(20):\n",
    "    start_time = time.time()\n",
    "    one_prediction = tf_sess.run(output_tensor, feed_dict)\n",
    "    delta = (time.time() - start_time)\n",
    "    times.append(delta)\n",
    "mean_delta = np.array(times).mean()\n",
    "fps = 1 / mean_delta\n",
    "print('average(sec):{:.2f},fps:{:.2f}'.format(mean_delta, fps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It got a 27.18 FPS which can be considered prediction in real time. In addition, the Keras model can inference at 60 FPS on Colab's Tesla K80 GPU, which is twice as fast as Jetson Nano, but that is a data center card."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 32-bit",
   "language": "python",
   "name": "python38232bit96d555843e3845678ef9c791f4256d4c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
